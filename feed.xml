<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://manuflores.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://manuflores.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-03T00:46:40+00:00</updated><id>https://manuflores.github.io/feed.xml</id><title type="html">blank</title><subtitle># </subtitle><entry><title type="html">A connection between the Markov and Laplace matrix from the lens of the Gershgorin disk theorem</title><link href="https://manuflores.github.io/blog/2022/laplace-markov/" rel="alternate" type="text/html" title="A connection between the Markov and Laplace matrix from the lens of the Gershgorin disk theorem"/><published>2022-11-02T00:00:00+00:00</published><updated>2022-11-02T00:00:00+00:00</updated><id>https://manuflores.github.io/blog/2022/laplace-markov</id><content type="html" xml:base="https://manuflores.github.io/blog/2022/laplace-markov/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Mathematics is all about connections. In this post I want to share with you a starkingly beautiful connection between the <a href="https://en.wikipedia.org/wiki/Stochastic_matrix">Markov</a> and <a href="https://en.wikipedia.org/wiki/Discrete_Laplace_operator#Graph_Laplacians">graph Laplacian matrix</a>. This was the result of trying to find a fast algorithm to visualize data that preserves certain nice properties of the underlying space. In a moment of eureka I payed closer attention to this connection and found that all I needed for my problem was an application of the Gershgorin domain theorem. This connection came some time after writing my previous post so I thought it would be a good follow-up.</p> <p>I would’ve liked to have written a short paper about this discovery, but this connection had already been reported in the literature in 2014 by the title <a href="https://www.bnl.gov/isd/documents/92821.pdf">Diverse Power Iteration Embeddings and Its Applications</a> by Huang et al. Since I cannot write a paper on it, I’ll indulge myself in the pleasure of explaining the discovery in plain and hopefully intuitive language.</p> <h3 id="motivation-and-rationale-data-visualization-and-clustering">Motivation and rationale: data visualization and clustering</h3> <p>I was using a lot the <a href="https://proceedings.neurips.cc/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf">Laplacian Embedding</a> in my research in order to visualize data. In a nutshell, the algorithm works by finding the smallest eigenvectors associated with the smallest (non-zero) eigenvalues of the <a href="https://en.wikipedia.org/wiki/Laplacian_matrix">Laplacian matrix</a> or graph Laplacian Matrix (we’ll get to the definition in a moment). This matrix is square and has number of rows and columns being equal to the number of datapoints. If you have millions or billions of datapoints, you can imagine that finding the eigenvectors of this matrix can be computationally expensive. I thought to myself that it was a waste of computation to get all of the eigenvectors–since all we need is the first three or so in order to visualize data, and at most on the order of hundreds if we needed to use it for clustering. This started my quest to finding a faster algorithm.</p> <p>At first I thought of using the <a href="https://en.wikipedia.org/wiki/Power_iteration">Power method</a> - since the flavor of the problem was a good fit. However, the power method is used for the largest eigenpairs - and remember we want the smaller vectors of the spectrum. This led me to the inverse power method which is explained in the classic Linear Algebra book by Gilbert Strang. However, I was not satisfied by this approach and remembered that in the <a href="https://arxiv.org/pdf/0711.0189.pdf">classic Spectral clustering tutorial paper by Ulrike von Luxburg</a>, she mentions connections between the spectrum of different versions of the graph Laplacian, one of which is computed from a Markov matrix, that is the row stochastic matrix associated with a random walk on an undirected graph. In her paper, Von Luxburg mentions the connection that the largest eigenvalues of the Markov matrix correspond to the smallest of the random walk graph Laplacian but doesn’t prove it. The same is true for the study of Huang et al.</p> <p>The above connection led me to try to prove this connection using the Gershgorin domain theorem.</p> <h3 id="preliminaries">Preliminaries</h3> <p>We start by using an adjacency matrix \(A\) that represents an undirected graph.</p> <p><strong>Definition</strong> Adjacency matrix \(A\). The construction for this matrix is straight-forward: it’s a binary symmetric matrix, with a 1 in the (i,j) position if node <em>i</em> is connected to node <em>j</em>. Thus, the adjacency matrix encodes the connectivity of a graph or network.</p> <p>You can build this matrix from data using for example a k-nearest neighbors (kNN) approach.</p> <p>Then, we need another very important matrix, the degree matrix \(D\) by summing across the rows of the adjacency matrix.</p> <p><strong>Definition</strong> Degree matrix.</p> <p>The degree matrix is diagonal with \(D_{ii} = \sum_{j=1}^n A_{ij}\), where \(A\) is the adjacency matrix.</p> <p>This matrix then represents the number of connections each node has.</p> <p>We define the associated Markov matrix as follows:</p> <p><strong>Definition:</strong> Markov matrix. The Markov transition matrix for an undirected graph is defined as \(P = D^{-1}A\), where \(D\) is the degree matrix and \(A\) is the adjacency matrix. We denote it \(P\) as it is a probabilistic matrix. More specifically, using the (i,j) entry of \(P\) we get the probability of visiting each node \(j\) in the graph starting from node \(i\). In other words we get the dynamics of a random walk on the graph!</p> <p><strong>Definition:</strong> Random walk Laplacian</p> <p>The random walk Laplacian is defined as:</p> \[\begin{align} L^{rw} = D^{-1}L = I - P \end{align}\] <p>where \(L = D - A\) is the unnormalized graph Laplacian. It turns out that the spectrum of \(L\) and \(L^{rw}\) coincide, as we’ll show in the next section.</p> <p>With these ingredients and with the definitions laid down in my previous post on the <a href="https://manuflores.github.io/gershgorin-theorem/">Gershgorin domain theorem</a> we can get to the core of this post!</p> <h3 id="theorems">Theorems</h3> <p><strong>Proposition</strong> Markov and Laplace eigenvectors coincide.</p> <p><em>Proof</em>: We show this for the case of the random walk Laplacian. We directly show this by expanding the eigenvector-eigenvalue relation of each of these matrices.</p> \[\begin{align} Lv = (I-P)v = v - Pv = \lambda v \\ \implies Pv = v - \lambda v \\ \implies Pv = (1 - \lambda)v \end{align}\] <p>More importantly the smallest eigenvalue of the Laplacian is exactly the largest eigenvalue of the Markov as we show next. But first, we need to bound the eigenvalues of the Markov matrix since this will give us bounds on the Laplace spectrum - which is what we ultimately want.</p> <p><strong>Lemma</strong>: Markov spectrum is bounded to the unit disk of the complex plane.</p> <p><em>Proof</em>: This follows by application of the Gershgorin domain theorem. Since the rows sum to one, the radius of each Gershgorin disk cannot exceed the one. Let \(D_i\) be the i-th Gershgorin disk</p> <ul> <li>Case 1: If \(P_{ii} = 0 \implies D_i = B_{1}(0) \subset \mathbb{C}\), that is, if the diagonal entry \(i\) of the markov matrix is 0, then the sum of the off diagonal entries of the row sum to one and thus the corresponding Gershgorin disk is the unit closed ball centered at zero.</li> <li>Case 2: If \(P_{ii} = 1 \implies D_i = B_{0}(1)\)</li> <li>Case 3 (general case) : If \(P_{ii} \in (0,1) \implies D_i = B_x(y)\) where \(x + y = 1, x,y \in (0,1)\).</li> </ul> <p>This concludes our proof.</p> <p>Thus, we just showed that <strong>the magnitude of the eigenvalues of a Markov matrix is less than or equal to one</strong>, i.e. that the spectrum of a Markov matrix \(P\) is bounded to the unit disk of the complex plane.</p> <p>We now bound the spectrum of the random walk graph Laplacian matrix.</p> <p><strong>Corollary</strong> \(\mathrm{spec} \, L^{rw} \subseteq 1 - \mathbb{D^2}\)</p> <p><em>Proof</em>: The proof is a straightforward calculation:</p> \[\begin{align} \lambda_L^{rw} &amp;= 1 - \lambda_P \\ \implies \mathrm{spec} \, L^{rw} &amp;\subseteq 1 - \mathrm{spec} \, P\\ &amp;\subseteq 1 - \mathbb{D^2} \subseteq \mathbb{C} \end{align}\] <h2 id="geometric-interpretation-of-the-connection-between-the-spectrum-of-laplace-and-markov-matrices">Geometric interpretation of the connection between the spectrum of Laplace and Markov matrices</h2> <p>It’s a good moment to step back and think about what we have just arrived and what are the consequences for our goal. For this, I’d like to bring some pictures to visualize our results.</p> <p>First, the Gershgorin disk theorem tells us that the spectrum of a Markov matrix lives in the unit disk. For visualization, let’s look at the vectors in the unit circle and color them by their angle.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vectors_in_unit_circle-480.webp 480w,/assets/img/vectors_in_unit_circle-800.webp 800w,/assets/img/vectors_in_unit_circle-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/vectors_in_unit_circle.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>But where does the spectrum of the Laplacian live in the complex plane?</p> <p>We have the equation \(\lambda_L = 1 - \lambda_p\). This remarkably beautiful and simple formula puzzled me for a while, since written in set notation \(\mathrm{spec} \, L \subseteq 1 - \mathbb{D^2}\). Let’s break this down.</p> <p>First, the negative of a complex number is just the negative of the real and the imaginary part. Thus we can think of multuplying a complex number by -1 as the action of the linear transformation \(\mathrm{diag}(-1,-1)\). We can visualize the operation of the negative of vectors in the complex unit circle as follows:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flipped_vectors_in_unit_circle-480.webp 480w,/assets/img/flipped_vectors_in_unit_circle-800.webp 800w,/assets/img/flipped_vectors_in_unit_circle-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/flipped_vectors_in_unit_circle.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Finally, to get the set \(1 - \mathbb{D^2}\) we just need to <em>shift</em> the disk one unit to the right in the real component of the complex plane (horizontal direction). We can visualize the Gershgorin domains as follows (follow the colors to see where each original vector lands after the transformation!):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spectrum_laplace_markov-480.webp 480w,/assets/img/spectrum_laplace_markov-800.webp 800w,/assets/img/spectrum_laplace_markov-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/spectrum_laplace_markov.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Importantly, let’s imagine following the eigenvalue 1 of Markov. Our formula tells us that the corresponding eigenvalue of the Laplace matrix would be zero! Geometrically, we first would have to multiply it by -1 which would flip it to the vector \((-1,0)^T\) in the complex plane and then shifting it to the right by a unit which would take it to the origin in the complex plane, i.e. the complex number 0.</p> <p>If you’re following along, you can guess now where the largest eigenvalues of the Laplace matrix will land w.r.t. to the corresponding eigenvalues of its Markov matrix. Don’t worry if you don’t – the whole idea of dealing with the complex plane can be confusing. But it’s important to think about, as Markov matrices can indeed have imaginary eigenvalues.</p> <p>Luckily for us we can bound the spectrum to the real numbers: it is a fact that if a matrix is symmetric by the Spectral theorem this implies that the eigenvalues of the matrix are real. Furthermore if a matrix is positive semidefinite its eigenvalues are non-negative. This is the last ingredient which helps us bound the eigenvalues of the Laplacian matrix and give a direct segway to our algorithm.</p> <h2 id="the-final-step--keeping-it-real">The final step : keeping it real</h2> <p>We now have an explicit bounding region for which the eigenvalues of the Laplacian live. Note that we have been working with the random walk Laplacian to make the connection to Markov matrices. We now define an alternative version of the graph Laplacian:</p> <p><strong>Definition</strong> The Symmetric graph Laplacian is defined as \(L^s = D^{-1/2} \, L \, D^{-1/2}\).</p> <p>The above definition is helpful for inheriting the properties of a symmetric matrix, and especially for our discussion the fact that symmetric matrices have real eigenvalues. To see why this is important for us note that : \(D^{-1/2} \, ( L^s )\, D^{1/2} = D^{-1/2} \, ( D^{-1/2} \, L \, D^{-1/2}) \, D^{1/2} = D^{-1} \, L = L^{rw}\)</p> <p>In fancy mathematical language this shows that the Symmetric graph Laplacian and the Random Walk Laplacian are <a href="https://en.wikipedia.org/wiki/Matrix_similarity#Properties">similar</a>. Thus, they have the same spectrum. <strong>This means that the eigenvalues of the Random Walk Laplacian are also all real</strong>, since the symmetric Laplacian has only real eigenvalues by the Spectral theorem.</p> <p>This leads to the following refinement of our last corollary.</p> <p><strong>Corollary (refinement)</strong> \(\mathrm{spec} \, L^{rw} \subseteq [0,2]\)</p> <p>Using this last statement, we can finally see why the <em>largest eigenvalues of the Markov matrix correspond to the smallest eigenvalues of the Laplacian matrix</em>. For the sake of our argument, let’s also focus on symmetrized markov matrices: \(P^{s} = \frac{1}{2} ( P + P^T)\). Using this construction, the spectrum of symmetric Markov matrices will be a subset of the interval \([-1,1] \subset \mathbb{R}\). We can now talk about smallest and largest in the ordering sense.</p> <p>Using the equation \(\lambda_L = 1 - \lambda_P\), we can see that thinking of the spectrums as ordered sets, the negative sign <strong>flips the order</strong> of the eigenvalues from largest to smallest and viceversa, and then shifts the intervals by 1 to the right. It helped me to imagine doing the operation by having numpy arrays and imagine taking a linearly increasing sample of the interval \([-1,1]\) and then just applying our operation.</p> <p>With this in mind, we can finally write down our algorithm for fast visualization using the Laplacian embedding.</p> <h3 id="algorithm--markov-laplace-embedding">Algorithm : Markov-Laplace embedding</h3> <ol> <li>Construct a graph from data, and store it in adjacency matrix \(A\).</li> <li>Compute the Markov matrix \(P\) associated with \(A\).</li> <li>Compute the first three largest eigenvectors of \(P\) using the <a href="https://en.wikipedia.org/wiki/Power_iteration">Power method</a>.</li> <li>Discard the first one (associated with eigenvalue 1 of Markov).</li> <li>Visualize data in vectors 2 and 3.</li> </ol> <p>This is a fast method to visualize data using the Laplacian Embedding. If you set a random seed, then your embeddings will be deterministic. Enjoy!</p> <h3 id="the-laplacian-matrix-and-connections-to-other-concepts-in-math">The Laplacian matrix and connections to other concepts in math</h3> <p>The Laplacian matrix is one of the mathematical objects which I’ve found to have some of the most fascinating connections to different geometrical and physical phenomena. These are some of my favorite:</p> <ul> <li> <p>The <a href="https://en.wikipedia.org/wiki/Discrete_Laplace_operator#Discrete_heat_equation">solutions of the heat equation in a graph</a></p> </li> <li> <p><a href="https://www.math.ucdavis.edu/~hunter/m207b/kac.pdf">Hearing the shape of a drum</a> (or can we?)</p> </li> <li> <p><a href="https://members.loria.fr/Bruno.Levy/papers/Laplacian_SMI_2006.pdf">Transferring the pose of an armadillo to Homer simpson</a>.</p> </li> </ul> <h3 id="final-note--alternative-proof-for-the-upper-bound-on-the-eigenvalues-of-the-laplacian-graph">Final note : alternative proof for the upper bound on the eigenvalues of the Laplacian graph</h3> <p>Not too long ago (at most a couple of years) I came across the classic notes of <a href="https://mathweb.ucsd.edu/~fan/cbms.pdf">Spectral Graph Theory from Fan Chung</a>, and I remember not being able to follow the proof for the Lemma 1.7(v) that states essentially that the eigenvalues of the graph Laplacian are smaller than or equal to 2. This is an alternative proof for that statement using a different approach.</p> <p>To see this note that our proof focused on the random walk Laplacian, but the eigenvalues coincide with those of the classic graph Laplacian.</p> <h3 id="acknowledgements">Acknowledgements</h3> <p>I’d like to especially thank to my advisor Matt Thomson for introducing me into the rigor of mathematics and particularly the appreciation of Linear Algebra.</p> <p>Secondly, I am massively grateful to Kostia Zuev for his lectures on Applied Linear Algebra. This post is a direct consequence of taking his course, ACM 104.</p> <p>Finally, I’d like to thank my friend and collaborator Alejandro Granados. This idea was developed while working under his supervision at the CZ Biohub Summer Internship in Angela Pisco’s Data Science team.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Bounding eigenvalues of a matrix using the Gershgorin disk theorem</title><link href="https://manuflores.github.io/blog/2021/gershgorin-theorem/" rel="alternate" type="text/html" title="Bounding eigenvalues of a matrix using the Gershgorin disk theorem"/><published>2021-11-11T00:00:00+00:00</published><updated>2021-11-11T00:00:00+00:00</updated><id>https://manuflores.github.io/blog/2021/gershgorin-theorem</id><content type="html" xml:base="https://manuflores.github.io/blog/2021/gershgorin-theorem/"><![CDATA[<h2 id="the-beauty-of-the-gershgorin-disk-theorem">The beauty of the Gershgorin disk theorem</h2> <p>In this post I’ll talk about one of the most beatiful theorems I’ve encountered while studying linear algebra. I bumped into it while taking the ACM104 Applied Linear Algebra course at Caltech, taught by Prof. Kostia Zuev. As the name of the course suggests, this theorem has a lot of applications, one of which we will explore in the end, so if you’re here for the applications, read on! The math can seem a bit hairy if you skim over quickly, but I assure you that it is actually surprisingly simple.</p> <p><em>Disclaimer</em>: The “applications” in the bottom of this post are to understand better certain groups of matrices. For a “real application” of the theorem, in my next post I will use it for an actual algorithm you can use to visualize your data. If you’re more interested in the latter, hang tight.</p> <p>I find this theorem aesthetically pleasing because it has a visual representation. Moreover, I think that the wit with which it came about (by the <a href="https://en.wikipedia.org/wiki/Semyon_Aranovich_Gershgorin">short-lived mathematician Semyon Gershgorin</a>) is to be praised for. It is a clear example of how mathematics are <em>just right there</em>, waiting to be discovered.</p> <p>We follow the proof of the book <em>Applied Linear Algebra</em> by Olver and Shakiban.</p> <h2 id="preliminaries">Preliminaries</h2> <p>Before stating the theorem, we’ll need some definitions. Recall that the magnitude of a complex number \(z = a + ib\) is defined by</p> \[|z| = \sqrt{a^2 + b^2}\] <p>If we define the conjugate, we can compute the magnitude as follows:</p> \[\bar{z} = a - ib, |z|^2 = z \bar{z}\] <p><strong>Definition.</strong> <em>Gershgorin disk</em>. Let \(A \in \mathbb{M}_{n \times n}\) a matrix over \(\mathbb{F}\) (either \(\mathbb{R}\) or \(\mathbb{C}\)).</p> <p>For each \(1 \le i \le n\), define the \(i\)-th Gershgorin disk as :</p> \[\begin{align} D_i = \{ |A_{ii} - z_i | \leq r_i : z \in \mathbb{C} \}\\ r_i = \sum_{j = 1, j \neq i} | A_{ij} | \end{align}\] <p>I know this definition can be not quite intuitive so let me break it down. In words, the i-th Gershgorin disk of a square matrix A is just a closed <a href="https://en.wikipedia.org/wiki/Ball_(mathematics)">ball</a> centered at \(A_{ii}\) with radius equal to the sum of the absolute value of the off-diagonal elements of the i-th row. By construction this ball will live in the complex plane. To keep things simple and start building a mental picture of the theorem, you can think of a single Gershgorin disk as simply a disk or closed ball (a filled circle) in the Cartesian plane.</p> <p><strong>Definition.</strong> <em>Gershgorin domain</em>. The union of the \(n\) Gershgorin disks is the Gershgorin domain.</p> \[\mathfrak{D}_A = \{ \cup_{i = 1}^n D_i \} \subseteq \mathbb{C}\] <p>We visualize the Gershgorin domain of a matrix in the image below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gersh-480.webp 480w,/assets/img/gersh-800.webp 800w,/assets/img/gersh-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gersh.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Definition.</strong> <em>Spectrum of a matrix</em>. We call the spectrum of a matrix A to the set of eigenvalues associated with A. We denote it as \(\mathrm{spec} A\).</p> <h2 id="the-statement">The statement</h2> <p><strong>Theorem.</strong> The spectrum of a matrix A lies within the Gershgorin domain.</p> \[\mathrm{spec A} \subseteq \mathfrak{D}_A \subseteq \mathbb{C}\] <p><em>Proof</em>: The constructive proof is surprisingly straightforward.</p> <p>Let \(\lambda\) be an eigenvalue of A, \(\vec{v}\) be its associated eigenvector. Let \(\vec{u} = \frac{v}{ || v ||_{\infty} }\) be the corresponding unit eigenvector with respect to (w.r.t.) the \(\infty\) norm, i.e.</p> \[|| u ||_\infty = \mathrm{max} \{ |u_1|, ..., |u_n| \} = 1\] <p>Let \(u_i\) be an entry of \(\vec{u}\) that achieves the maximum \(\mid u_i \mid = 1\). Writing out the <em>i</em>-th component of eigenvalue equation we obtain:</p> \[\sum_{j = 1}^ n \mathbf{A}_{ij} u_j = \lambda u_i\] <p>which we can rewrite as:</p> \[\sum_{j \neq i } \mathbf{A}_{ij} u_j = \lambda u_i - \mathbf{A}_{ii} u_i = (\lambda - \mathbf{A}_{ii} ) u_i\] <p>Note: in the last step we just subtracted \(\mathbf{A}_{ii}u_i\) from both sides.</p> <p>Thus, since all \(\mid u_j \mid \le 1\) while \(\mid u_i \mid = 1\) we have that</p> \[\begin{align} |\mathbf{A}_ii - \lambda| &amp;= |\lambda - \mathbf{A}_{ii}| \\ &amp;= |\lambda - \mathbf{A}_{ii}| |u_i|\\ &amp;= | (\lambda - \mathbf{A}_{ii}) u_i | \\ &amp;= | \sum_{j \neq i} \mathbf{A}_{ij} u_j | \\ &amp;\le \sum_{j \neq i } |\mathbf{A}_{ij}| |u_j| \\ &amp;\le \sum_{j \neq i} |\mathbf{A}_{ij}|\\ &amp;= r_i. \end{align}\] <p>We have equality in the third step as can be checked for all cases ( ++, +-, -+, – ). The fourth step is just substituting the equation above. The fifth step holds by the triangle inequality \(||x+y|| \le ||x|| + ||y||\). The sixth line holds as \(|u_j| \le 1 \forall j \neq i\) by construction of \(\vec{u}\).</p> <p>This implies the following:</p> \[|\lambda - \mathbf{A}_{ii}| \le r_i \implies \lambda \in D_i\] <p>An immediate corollary is related to the invertibility of square matrices.</p> <p><strong>Definition.</strong> A square matrix is called diagonally dominant if</p> \[|a_{ii}| &gt; \sum_{j \neq i} |a_{ij}| \forall i = 1, ..., n.\] <p><strong>Corollary.</strong> A strictly diagonally dominant matrix is nonsingular.</p> <p><em>Proof</em>: A matrix is singular iff it admits zero as an eigenvalue (an eigenspace shrinks to zero \(\implies \mathrm{dim }\, \mathrm{ker} \ge 1\). ). Thus, if its Gershgorin domain doesn’t contain zero, it cannot be an eigenvalue, hence A is necessarily invertible / non-singular.</p> <p><strong>Theorem.</strong> A symmetric matrix is positive definite if all of its eigenvalues are positive.</p> <p><strong>Corollary.</strong> A symmetric matrix is positive definite if its Gershgorin domain lies in the positive side of the \(\mathbb{C}\) plane. In other words, a matrix is p.d. if \(a_{ii} &gt; \sum_{j\neq i} a_i \forall i = 1, ..., n.\).</p> <h2 id="examples">Examples</h2> <p><a href="https://colab.research.google.com/github/manuflores/sandbox/blob/master/notebooks/gershgorin.ipynb">Here are some visualizations in a jupyter colab notebook</a> if you want to get a feel of the theorem.</p>]]></content><author><name>Emanuel Flores</name></author><category term="math"/><summary type="html"><![CDATA[A method to approximate the spectrum of a matrix.]]></summary></entry></feed>