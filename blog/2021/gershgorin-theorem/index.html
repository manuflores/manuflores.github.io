<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <hr> <p>layout: distill title: Bounding eigenvalues of a matrix using the Gershgorin disk theorem description: date: 2021-11-11 featured: true</p> <p>authors: - name : Emanuel Flores affiliations: name: Caltech</p> <h1 id="toc">toc:</h1> <h1 id="--name-origins">- name: Origins</h1> <h1 id="-if-a-section-has-subsections-you-can-add-them-as-follows"># if a section has subsections, you can add them as follows:</h1> <h1 id="-subsections"># subsections:</h1> <h1 id="-----name-example-child-subsection-1"># - name: Example Child Subsection 1</h1> <h1 id="-----name-example-child-subsection-2"># - name: Example Child Subsection 2</h1> <h1 id="--name-law-of-large-numbers-and-frequentism">- name: Law of large numbers and frequentism</h1> <h1 id="--name-statistical-mechanics">- name: Statistical mechanics</h1> <h1 id="--name-hilberts-sixth-problem">- name: Hilbert’s sixth problem</h1> <h1 id="--name-axiomatization-in-the-elementary-finite-case">- name: Axiomatization in the “elementary” (finite) case</h1> <h1 id="--name-infinite-families-of-events">- name: Infinite families of events</h1> <h3 id="the-beauty-of-the-gershgorin-disk-theorem">The beauty of the Gershgorin disk theorem</h3> <p>In this post I’ll talk about one of the most beatiful theorems I’ve encountered while studying linear algebra. I bumped into it while taking the ACM104 Applied Linear Algebra course at Caltech, taught by Prof. Kostia Zuev. As the name of the course suggests, this theorem has a lot of applications, one of which we will explore in the end, so if you’re here for the applications, read on! The math can seem a bit hairy if you skim over quickly, but I assure you that it is actually surprisingly simple.</p> <p><em>Disclaimer</em>: The “applications” in the bottom of this post are to understand better certain groups of matrices. For a “real application” of the theorem, in my next post I will use it for an actual algorithm you can use to visualize your data. If you’re more interested in the latter, hang tight.</p> <p>I find this theorem aesthetically pleasing because it has a visual representation. Moreover, I think that the wit with which it came about (by the <a href="https://en.wikipedia.org/wiki/Semyon_Aranovich_Gershgorin" rel="external nofollow noopener" target="_blank">short-lived mathematician Semyon Gershgorin</a>) is to be praised for. It is a clear example of how mathematics are <em>just right there</em>, waiting to be discovered.</p> <p>We follow the proof of the book <em>Applied Linear Algebra</em> by Olver and Shakiban.</p> <h3 id="preliminaries">Preliminaries</h3> <p>Before stating the theorem, we’ll need some definitions. Recall that the magnitude of a complex number \(z = a + ib\) is defined by</p> \[|z| = \sqrt{a^2 + b^2}\] <p>If we define the conjugate, we can compute the magnitude as follows:</p> \[\bar{z} = a - ib, |z|^2 = z \bar{z}\] <p><strong>Definition.</strong> <em>Gershgorin disk</em>. Let \(A \in \mathbb{M}_{n \times n}\) a matrix over \(\mathbb{F}\) (either \(\mathbb{R}\) or \(\mathbb{C}\)).</p> <p>For each \(1 \le i \le n\), define the \(i\)-th Gershgorin disk as :</p> \[\begin{align} D_i = \{ |A_{ii} - z_i | \leq r_i : z \in \mathbb{C} \}\\ r_i = \sum_{j = 1, j \neq i} | A_{ij} | \end{align}\] <p>I know this definition can be not quite intuitive so let me break it down. In words, the i-th Gershgorin disk of a square matrix A is just a closed <a href="https://en.wikipedia.org/wiki/Ball_(mathematics)" rel="external nofollow noopener" target="_blank">ball</a> centered at \(A_{ii}\) with radius equal to the sum of the absolute value of the off-diagonal elements of the i-th row. By construction this ball will live in the complex plane. To keep things simple and start building a mental picture of the theorem, you can think of a single Gershgorin disk as simply a disk or closed ball (a filled circle) in the Cartesian plane.</p> <p><strong>Definition.</strong> <em>Gershgorin domain</em>. The union of the \(n\) Gershgorin disks is the Gershgorin domain.</p> \[\mathfrak{D}_A = \{ \cup_{i = 1}^n D_i \} \subseteq \mathbb{C}\] <p><strong>Definition.</strong> <em>Spectrum of a matrix</em>. We call the spectrum of a matrix A to the set of eigenvalues associated with A. We denote it as \(\mathrm{spec} A\).</p> <h3 id="the-statement">The statement</h3> <p><strong>Theorem.</strong> The spectrum of a matrix A lies within the Gershgorin domain.</p> \[\mathrm{spec A} \subseteq \mathfrak{D}_A \subseteq \mathbb{C}\] <p><em>Proof</em>: The constructive proof is surprisingly straightforward.</p> <p>Let \(\lambda\) be an eigenvalue of A, \(\vec{v}\) be its associated eigenvector. Let \(\vec{u} = \frac{v}{ || v ||_{\infty} }\) be the corresponding unit eigenvector with respect to (w.r.t.) the \(\infty\) norm, i.e.</p> \[|| u ||_\infty = \mathrm{max} \{ |u_1|, ..., |u_n| \} = 1\] <p>Let \(u_i\) be an entry of \(\vec{u}\) that achieves the maximum \(\mid u_i \mid = 1\). Writing out the <em>i</em>-th component of eigenvalue equation we obtain:</p> \[\sum_{j = 1}^ n \mathbf{A}_{ij} u_j = \lambda u_i\] <p>which we can rewrite as:</p> \[\sum_{j \neq i } \mathbf{A}_{ij} u_j = \lambda u_i - \mathbf{A}_{ii} u_i = (\lambda - \mathbf{A}_{ii} ) u_i\] <p>Note: in the last step we just subtracted \(\mathbf{A}_{ii}u_i\) from both sides.</p> <p>Thus, since all \(\mid u_j \mid \le 1\) while \(\mid u_i \mid = 1\) we have that</p> \[\begin{align} |\mathbf{A}_ii - \lambda| &amp;= |\lambda - \mathbf{A}_{ii}| \\ &amp;= |\lambda - \mathbf{A}_{ii}| |u_i|\\ &amp;= | (\lambda - \mathbf{A}_{ii}) u_i | \\ &amp;= | \sum_{j \neq i} \mathbf{A}_{ij} u_j | \\ &amp;\le \sum_{j \neq i } |\mathbf{A}_{ij}| |u_j| \\ &amp;\le \sum_{j \neq i} |\mathbf{A}_{ij}|\\ &amp;= r_i. \end{align}\] <p>We have equality in the third step as can be checked for all cases ( ++, +-, -+, – ). The fourth step is just substituting the equation above. The fifth step holds by the triangle inequality \(||x+y|| \le ||x|| + ||y||\). The sixth line holds as \(|u_j| \le 1 \forall j \neq i\) by construction of \(\vec{u}\).</p> <p>This implies the following:</p> \[|\lambda - \mathbf{A}_{ii}| \le r_i \implies \lambda \in D_i\] <p>An immediate corollary is related to the invertibility of square matrices.</p> <p><strong>Definition.</strong> A square matrix is called diagonally dominant if</p> \[|a_{ii}| &gt; \sum_{j \neq i} |a_{ij}| \forall i = 1, ..., n.\] <p><strong>Corollary.</strong> A strictly diagonally dominant matrix is nonsingular.</p> <p><em>Proof</em>: A matrix is singular iff it admits zero as an eigenvalue (an eigenspace shrinks to zero \(\implies \mathrm{dim }\, \mathrm{ker} \ge 1\). ). Thus, if its Gershgorin domain doesn’t contain zero, it cannot be an eigenvalue, hence A is necessarily invertible / non-singular.</p> <p><strong>Theorem.</strong> A symmetric matrix is positive definite if all of its eigenvalues are positive.</p> <p><strong>Corollary.</strong> A symmetric matrix is positive definite if its Gershgorin domain lies in the positive side of the \(\mathbb{C}\) plane. In other words, a matrix is p.d. if \(a_{ii} &gt; \sum_{j\neq i} a_i \forall i = 1, ..., n.\).</p> <h3 id="examples">Examples</h3> <p><a href="https://colab.research.google.com/github/manuflores/sandbox/blob/master/notebooks/gershgorin.ipynb" rel="external nofollow noopener" target="_blank">Here are some visualizations in a jupyter colab notebook</a> if you want to get a feel of the theorem.</p> </body></html>